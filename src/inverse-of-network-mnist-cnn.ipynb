{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_PATH = Path('../inputs/mnist/cnn')\n",
    "OUTPUT_PATH = Path('../outputs/mnist/cnn')\n",
    "\n",
    "INPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv outs=torch.Size([1, 10, 13, 13]), torch.Size([1, 20, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 10, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Linear(in_features=500, out_features=250, bias=True)\n",
       "    (3): Linear(in_features=250, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(999)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, in_channel, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        layer1 = self.get_conv_relu_maxpool(in_channel, 10, 2, 2)\n",
    "        layer2 = self.get_conv_relu_maxpool(10, 20, 3, 2)\n",
    "        layer1_out = layer1(torch.rand((1, 1, 28, 28)))\n",
    "        layer2_out = layer2(layer1_out)\n",
    "        print(f'conv outs={layer1_out.size()}, {layer2_out.size()}')\n",
    "        layer3 = nn.Linear(layer2_out.flatten(1).size(-1), int(layer2_out.flatten(1).size(-1)/2))\n",
    "        layer4 = nn.Linear(int(layer2_out.flatten(1).size(-1)/2), out_size)\n",
    "        self.layers = nn.ModuleList([layer1, layer2, layer3, layer4])\n",
    "        self.forward_vals = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.forward_vals.clear()\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear) and len(x.size()) > 2:\n",
    "                x = x.flatten(1)\n",
    "            x = layer(x)\n",
    "            self.forward_vals.append(x)\n",
    "        return x\n",
    "    \n",
    "    def get_conv_relu_maxpool(self, in_channel, num_filters, kernel_size, pool_size):\n",
    "        return nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.MaxPool2d(pool_size))\n",
    "    \n",
    "    def get_linear_with_relu(self, inp, out):\n",
    "        return nn.Sequential(nn.Linear(inp, out), nn.ReLU())\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "train_kwargs = {'batch_size': batch_size,\n",
    "               'shuffle': True}\n",
    "test_kwargs = {'batch_size': batch_size,\n",
    "              'shuffle': True}\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "dataset1 = datasets.MNIST(INPUT_PATH/'mnist', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST(INPUT_PATH/'mnist', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net(28, 1, 10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60032"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not already trained a model, uncomment the next cell and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# def train(model, device, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         # print(data.size(), data.view(data.size(0), -1).size())\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = F.cross_entropy(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "# def test():\n",
    "#     with torch.no_grad():\n",
    "#         n_correct = 0\n",
    "#         n_samples = 0\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             # max returns (value ,index)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             n_samples += labels.size(0)\n",
    "#             n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     acc = 100.0 * n_correct / n_samples\n",
    "#     print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "#     return acc\n",
    "\n",
    "\n",
    "# best_acc = -1\n",
    "# best_model = None\n",
    "# for epoch in range(1, 20):\n",
    "#     train(model, device, train_loader, optimizer, epoch)\n",
    "#     acc = test()\n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         best_model = copy.deepcopy(model)\n",
    "#         print(f'new best acc={best_acc}')\n",
    "#     else:\n",
    "#         print(f'current acc={acc}, prev_best_acc={best_acc}')\n",
    "\n",
    "# state = {\n",
    "#     'model_state': best_model.state_dict(),\n",
    "#     'test_acc': best_acc\n",
    "# }\n",
    "# torch.save(state, OUTPUT_PATH/'state_4layer_nn.pt')\n",
    "# print('best model saved')\n",
    "# 98.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state', 'test_acc'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load(OUTPUT_PATH/'state_4layer_nn.pt')\n",
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 10, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Linear(in_features=500, out_features=250, bias=True)\n",
       "    (3): Linear(in_features=250, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state['model_state'])\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "                                 \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'epochs': 300,\n",
    "    'alpha': 1,\n",
    "    'lr': 1e-2,\n",
    "    'lr_sched': 'exp',\n",
    "    'gamma': 0.999,\n",
    "    'loss': 'abs',\n",
    "    'min_lr': 1e-5,\n",
    "    'leaky_relu': 0.4,\n",
    "    'batch_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats():\n",
    "    train_qmnist = datasets.QMNIST(INPUT_PATH/'qmnist', train = True, download= True,\n",
    "                             transform=transforms.ToTensor())\n",
    "    dataloader = torch.utils.data.DataLoader(train_qmnist, batch_size=128)\n",
    "    nimages = 0\n",
    "    mean = 0.0\n",
    "    var = 0.0\n",
    "    for i_batch, batch_target in enumerate(dataloader):\n",
    "        batch = batch_target[0]\n",
    "        # Rearrange batch to be the shape of [B, C, W * H]\n",
    "        batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "        # Update total number of images\n",
    "        nimages += batch.size(0)\n",
    "        # Compute mean and std here\n",
    "        mean += batch.mean(2).sum(0) \n",
    "        var += batch.var(2).sum(0)\n",
    "\n",
    "    mean /= nimages\n",
    "    var /= nimages\n",
    "    std = torch.sqrt(var)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 13, 13])\n",
      "torch.Size([1, 20, 5, 5])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand((1, 1, 28, 28)).to(device))\n",
    "[print(val.size()) for val in model.forward_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of inv data=Dataset QMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ../inputs/mnist/cnn/qmnist\n",
      "    Split: train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 250]),\n",
       " torch.Size([2, 500]),\n",
       " torch.Size([2, 10, 13, 13]),\n",
       " torch.Size([2, 1, 28, 28])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InvNet(nn.Module):\n",
    "    def __init__(self, out_dim, out_channel, tgt_model):\n",
    "        super(InvNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i, tgt_layer in enumerate(tgt_model.layers[::-1], 1):\n",
    "            if isinstance(tgt_layer, nn.Sequential):\n",
    "                tgt_layer = tgt_layer[0]\n",
    "            \n",
    "            if isinstance(tgt_layer, nn.Linear):\n",
    "                if len(tgt_model.forward_vals[-i-1].size()) > 2:\n",
    "                    out_size = tgt_model.forward_vals[-i-1].flatten(1).size(-1)\n",
    "                    self.first_conv_dim = tgt_model.forward_vals[-i-1].size()\n",
    "                else:\n",
    "                    out_size = tgt_model.forward_vals[-i-1].size(-1)\n",
    "                layer = self.get_linear_with_relu(tgt_model.forward_vals[-i].size(-1), out_size)\n",
    "            elif isinstance(tgt_layer, nn.Conv2d):\n",
    "                if i < len(tgt_model.layers):\n",
    "                    out_channel = tgt_model.forward_vals[-i-1].size(1)\n",
    "                    layer = self.get_conv_relu(tgt_model.forward_vals[-i].size(1), out_channel, 3, 1, 5, 1, True)\n",
    "                else:\n",
    "                    out_channel = 1\n",
    "                    layer = self.get_conv_relu(tgt_model.forward_vals[-i].size(1), out_channel, 2, 1, 8, 1, False)\n",
    "                \n",
    "            layers.append(layer)\n",
    "\n",
    "        self.conv_type = type(layers[-1][0])\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.forward_vals = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.forward_vals.clear()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer[0], self.conv_type) and len(x.size()) <= 2:\n",
    "                x = x.view(x.size(0), self.first_conv_dim[1], self.first_conv_dim[2], self.first_conv_dim[3])  # out dims of conv2d of target model\n",
    "            x = layer(x)\n",
    "            self.forward_vals.append(x)\n",
    "        return x\n",
    "    \n",
    "    def get_conv_relu(self, in_channel, num_filters, kernel_size, stride, padding, dilation, use_actv=False):\n",
    "        if not use_actv:\n",
    "            return nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size, \n",
    "                                                    stride=stride, padding=padding, dilation=dilation,\n",
    "                                                    bias=True),\n",
    "                                )\n",
    "        else:\n",
    "            return nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size, \n",
    "                                                    stride=stride, padding=padding, dilation=dilation,\n",
    "                                                    bias=True),\n",
    "                                 nn.LeakyReLU(config_dict['leaky_relu']),\n",
    "                                )\n",
    "    \n",
    "    def get_linear_with_relu(self, inp, out):\n",
    "        return nn.Sequential(nn.Linear(inp, out), nn.LeakyReLU(config_dict['leaky_relu']))\n",
    "\n",
    "\n",
    "inv_model = InvNet(28, 1, model)\n",
    "inv_model.to(device)\n",
    "# mean, std = get_stats()\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "batch_size = config_dict['batch_size']\n",
    "test_kwargs = {'batch_size': batch_size,\n",
    "              'shuffle': True}\n",
    "train_qmnist = datasets.QMNIST(INPUT_PATH/'qmnist', train = True, download= True,\n",
    "                             transform=transform) \n",
    "test_qmnist = datasets.QMNIST(INPUT_PATH/'qmnist', train = False, download= True,\n",
    "                             transform=transform)\n",
    "\n",
    "print(f'size of inv data={train_qmnist}')\n",
    "inv_data_loader = torch.utils.data.DataLoader(train_qmnist, **test_kwargs)\n",
    "\n",
    "if config_dict['loss'] == 'mse':\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif config_dict['loss'] == 'huber':\n",
    "    loss_fn = nn.HuberLoss()\n",
    "else:\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(inv_model.parameters(), lr=config_dict['lr'])\n",
    "\n",
    "lr_sched = None\n",
    "if config_dict['lr_sched'] == 'exp':\n",
    "    lr_sched = optim.lr_scheduler.ExponentialLR(optimizer, config_dict['gamma'], verbose=False)\n",
    "\n",
    "# print(inv_model)\n",
    "inv_model(torch.rand((2, 10)).to(device))\n",
    "[val.size() for val in inv_model.forward_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = []\n",
    "# for batch, tgt in inv_data_loader:\n",
    "#     s.append((batch[0].unsqueeze(0), tgt[0]))\n",
    "#     s.append((batch[1].unsqueeze(0), tgt[1]))\n",
    "#     break\n",
    "# len(inv_data_loader), len(s)# inv_model(torch.rand((1, 10, )).to(device)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in s:\n",
    "#     print(t[0].size(), t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "for i in range(6):\n",
    "    example_data, example_target = test_qmnist[random.randint(0, len(test_qmnist))] # TODO for test only change dataloader !!!!!\n",
    "    ax = plt.subplot(2,3,i+1)\n",
    "    ax.set_title(f'label={example_target}')\n",
    "    plt.imshow(example_data.squeeze(), cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "tgt_to_inv_layer_map = {0: 3, 1: 2, 2: 1, 3: 0} # {0: 1, 1: 0}\n",
    "\n",
    "def train_inv(model, inv_model, device, data_loader, optimizer, epoch, alpha, layer_wise_only=False, layer_num=-1):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_loss_layer = 0\n",
    "    total_loss_img = 0\n",
    "    total_loss_cyc = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)  # torch.Size([128, 784]) torch.Size([128])\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        out = model(data)  \n",
    "        inv_out = inv_model(out)\n",
    "        \n",
    "        # step 1- layer wise load\n",
    "        loss_layer = layer_wise_loss(model, inv_model, data)\n",
    "        \n",
    "        if not layer_wise_only:\n",
    "            # step 2- upto layer-k loss\n",
    "            loss_img = k_layer_loss(model, inv_model, data)\n",
    "\n",
    "            # step 3- full network output loss\n",
    "            loss_cyc = cycle_consistency_inversion_loss(model, data, inv_out)\n",
    "        else:\n",
    "            loss_img = torch.tensor(0)\n",
    "            loss_cyc = torch.tensor(0)\n",
    "        \n",
    "        loss = loss_layer + loss_img + (alpha*loss_cyc)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_loss_layer += loss_layer.item()\n",
    "        total_loss_img += loss_img.item()\n",
    "        total_loss_cyc += loss_cyc.item()\n",
    "        \n",
    "    return total_loss / (batch_idx+1), total_loss_layer / (batch_idx+1), \\\n",
    "            total_loss_img / (batch_idx+1), total_loss_cyc / (batch_idx+1)\n",
    "\n",
    "\n",
    "def layer_wise_loss(model, inv_model, inp, k=-1):\n",
    "    loss = 0\n",
    "    for layer_num, layer in enumerate(model.layers[:k+1] if k > -1 else model.layers):\n",
    "        tgt_layer_in = inp if layer_num == 0 else model.forward_vals[layer_num-1]  # get the input tensor for this layer in tgt model\n",
    "        tgt_layer_out = model.forward_vals[layer_num]  # get the output of i-th target model layer \n",
    "        inv_layer = inv_model.layers[tgt_to_inv_layer_map[layer_num]]  # get the corresponding layer in reverse model\n",
    "        inv_layer_out = inv_layer(tgt_layer_out)  # feed the target model layer output to reverse model layer\n",
    "        if inv_layer_out.size(-1) != tgt_layer_in.size(-1):\n",
    "            tgt_layer_in = tgt_layer_in.flatten(1)\n",
    "        loss += loss_fn(inv_layer_out, tgt_layer_in)  # outputs of both layer should be similar\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def k_layer_loss(model, inv_model, inp, k=-1):\n",
    "    loss = 0\n",
    "    for layer_num, layer_ in enumerate(model.layers[:k+1] if k > -1 else model.layers):\n",
    "        tgt_layer_out = model.forward_vals[layer_num]\n",
    "        x = tgt_layer_out\n",
    "        for layer in inv_model.layers[-(1+layer_num):]:\n",
    "            if isinstance(layer[0], inv_model.conv_type) and len(x.size()) < 4:\n",
    "                x = x.view(x.size(0), inv_model.first_conv_dim[1], inv_model.first_conv_dim[2], inv_model.first_conv_dim[3])\n",
    "            x = layer(x)\n",
    "        loss += loss_fn(x, inp)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def cycle_consistency_inversion_loss(model, inp, inv_out, k=-1):\n",
    "    org_activations = copy.deepcopy(model.forward_vals[:k+1] if k > -1 else model.forward_vals)\n",
    "    out_for_inv_input = model(inv_out)  # pass the input image generated by inverted network\n",
    "    inv_activations = model.forward_vals[:k+1] if k > -1 else model.forward_vals\n",
    "    loss = 0\n",
    "    for org_actv, actv in zip(org_activations, inv_activations):\n",
    "        loss += loss_fn(actv, org_actv)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '4layer_conv2d_exp1e-2_ConvLeakyRelu'\n",
    "change_desc = 'gamma 0.999, minLr 1e-5, 4 layer models, conv2d, abs'\n",
    "is_resume = True\n",
    "run_path = OUTPUT_PATH/'runs'/run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnayash\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nayash/nn-are-reversible/runs/4layer_conv2d_exp1e-2_ConvLeakyRelu\" target=\"_blank\">4layer_conv2d_exp1e-2_ConvLeakyRelu</a></strong> to <a href=\"https://wandb.ai/nayash/nn-are-reversible\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/300 loss=6.2784, loss_layer=1.87605, loss_img=1.81883, loss_cyc=2.58351, epoch_duration=11 secs\n",
      "new best loss=6.2783992946275005\n",
      "epoch=1/300 loss=5.27042, loss_layer=1.48107, loss_img=1.77484, loss_cyc=2.01452, epoch_duration=11 secs\n",
      "new best loss=5.270423143911463\n",
      "epoch=2/300 loss=5.08367, loss_layer=1.41312, loss_img=1.77325, loss_cyc=1.8973, epoch_duration=11 secs\n",
      "new best loss=5.08367233845725\n",
      "epoch=3/300 loss=5.00414, loss_layer=1.38224, loss_img=1.77178, loss_cyc=1.85012, epoch_duration=11 secs\n",
      "new best loss=5.004136086526964\n",
      "epoch=4/300 loss=4.92958, loss_layer=1.36107, loss_img=1.77125, loss_cyc=1.79726, epoch_duration=11 secs\n",
      "new best loss=4.929575284661007\n",
      "epoch=5/300 loss=4.89849, loss_layer=1.35113, loss_img=1.77047, loss_cyc=1.77689, epoch_duration=11 secs\n",
      "new best loss=4.898486020468446\n",
      "epoch=6/300 loss=4.86564, loss_layer=1.34311, loss_img=1.7702, loss_cyc=1.75233, epoch_duration=11 secs\n",
      "new best loss=4.8656395727129125\n",
      "epoch=7/300 loss=4.84895, loss_layer=1.33905, loss_img=1.76973, loss_cyc=1.74018, epoch_duration=11 secs\n",
      "new best loss=4.848954820937948\n",
      "epoch=8/300 loss=4.82055, loss_layer=1.33373, loss_img=1.76966, loss_cyc=1.71716, epoch_duration=11 secs\n",
      "new best loss=4.820550469193123\n",
      "epoch=9/300 loss=4.81095, loss_layer=1.33309, loss_img=1.76986, loss_cyc=1.70799, epoch_duration=11 secs\n",
      "new best loss=4.810945405126381\n",
      "epoch=10/300 loss=4.80111, loss_layer=1.33017, loss_img=1.76992, loss_cyc=1.70102, epoch_duration=11 secs\n",
      "new best loss=4.801108734439939\n",
      "epoch=11/300 loss=4.79767, loss_layer=1.33282, loss_img=1.77021, loss_cyc=1.69464, epoch_duration=11 secs\n",
      "new best loss=4.7976677605846545\n",
      "epoch=12/300 loss=4.78478, loss_layer=1.32942, loss_img=1.76949, loss_cyc=1.68587, epoch_duration=11 secs\n",
      "new best loss=4.784781238417636\n",
      "epoch=13/300 loss=4.77733, loss_layer=1.32894, loss_img=1.76974, loss_cyc=1.67866, epoch_duration=11 secs\n",
      "new best loss=4.77733479074832\n",
      "epoch=14/300 loss=4.75519, loss_layer=1.32568, loss_img=1.77008, loss_cyc=1.65944, epoch_duration=11 secs\n",
      "new best loss=4.755193962471317\n",
      "epoch=15/300 loss=4.78702, loss_layer=1.33018, loss_img=1.76969, loss_cyc=1.68716, epoch_duration=11 secs\n",
      "epoch=16/300 loss=4.75921, loss_layer=1.32811, loss_img=1.77018, loss_cyc=1.66092, epoch_duration=11 secs\n",
      "epoch=17/300 loss=4.74614, loss_layer=1.32698, loss_img=1.76981, loss_cyc=1.64935, epoch_duration=11 secs\n",
      "new best loss=4.746138675380617\n",
      "epoch=18/300 loss=4.75075, loss_layer=1.32641, loss_img=1.76954, loss_cyc=1.65481, epoch_duration=11 secs\n",
      "epoch=19/300 loss=4.75764, loss_layer=1.32735, loss_img=1.76981, loss_cyc=1.66048, epoch_duration=11 secs\n",
      "epoch=20/300 loss=4.72905, loss_layer=1.3251, loss_img=1.76966, loss_cyc=1.63429, epoch_duration=11 secs\n",
      "new best loss=4.729051175147994\n",
      "epoch=21/300 loss=4.75741, loss_layer=1.3278, loss_img=1.77008, loss_cyc=1.65952, epoch_duration=11 secs\n",
      "epoch=22/300 loss=4.73592, loss_layer=1.32452, loss_img=1.76984, loss_cyc=1.64156, epoch_duration=11 secs\n",
      "epoch=23/300 loss=4.72835, loss_layer=1.32535, loss_img=1.77011, loss_cyc=1.63289, epoch_duration=11 secs\n",
      "new best loss=4.728346433212508\n",
      "epoch=24/300 loss=4.72664, loss_layer=1.3257, loss_img=1.77067, loss_cyc=1.63027, epoch_duration=11 secs\n",
      "new best loss=4.726639803538698\n",
      "epoch=25/300 loss=4.73288, loss_layer=1.32581, loss_img=1.76941, loss_cyc=1.63767, epoch_duration=11 secs\n",
      "epoch=26/300 loss=4.72082, loss_layer=1.32486, loss_img=1.76991, loss_cyc=1.62606, epoch_duration=11 secs\n",
      "new best loss=4.720822142131293\n",
      "epoch=27/300 loss=4.70954, loss_layer=1.32203, loss_img=1.76994, loss_cyc=1.61756, epoch_duration=11 secs\n",
      "new best loss=4.709539223327312\n",
      "epoch=28/300 loss=4.70326, loss_layer=1.32276, loss_img=1.7699, loss_cyc=1.61061, epoch_duration=11 secs\n",
      "new best loss=4.703264601449214\n",
      "epoch=29/300 loss=4.69411, loss_layer=1.31907, loss_img=1.76935, loss_cyc=1.6057, epoch_duration=11 secs\n",
      "new best loss=4.694108814572983\n",
      "epoch=30/300 loss=4.70993, loss_layer=1.32307, loss_img=1.76913, loss_cyc=1.61773, epoch_duration=11 secs\n",
      "epoch=31/300 loss=4.69711, loss_layer=1.32014, loss_img=1.76924, loss_cyc=1.60773, epoch_duration=11 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def round_(n, d=5):\n",
    "    return np.round(n, d)\n",
    "\n",
    "\n",
    "run_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EPOCHS = config_dict['epochs']\n",
    "ALPHA = config_dict['alpha']\n",
    "start_epoch = 0\n",
    "prev_best_loss = 999999\n",
    "total_training_time = 0\n",
    "\n",
    "state_path = run_path/'final_state_4layer_inv_nn.pt'\n",
    "\n",
    "if is_resume and state_path.exists():\n",
    "    state = torch.load(state_path)\n",
    "    inv_model.load_state_dict(state['model_state'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    if lr_sched:\n",
    "        lr_sched.load_state_dict(state['lr_sched'])\n",
    "    start_epoch = state['epoch']+1\n",
    "    prev_best_loss = state['prev_best_loss']\n",
    "\n",
    "    print(f'resuming training for {run_id} from epoch {start_epoch}, last loss {state[\"loss\"]} prev_best_loss {prev_best_loss}')\n",
    "    \n",
    "\n",
    "wandb.init(project='nn-are-reversible',\n",
    "           entity='nayash', save_code=True, id=run_id,\n",
    "           name=run_id, notes=change_desc,\n",
    "           dir=run_path,\n",
    "           resume='allow',\n",
    "           config=config_dict)\n",
    "wandb.watch(inv_model)\n",
    "\n",
    "if run_id in ['test', 'temp']:\n",
    "    os.environ['WANDB_MODE'] = 'offline'\n",
    "    print(f'run_id={run_id}, so wandb is disabled!')\n",
    "          \n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    stime = time.time()\n",
    "    total_loss, total_loss_layer, total_loss_img, total_loss_cyc = \\\n",
    "        train_inv(model, inv_model, device, inv_data_loader, optimizer, epoch, ALPHA) # TODO for test only !!!!!!\n",
    "\n",
    "    epoch_time = time.time() - stime\n",
    "    total_training_time += epoch_time\n",
    "    \n",
    "    print(f'epoch={epoch}/{EPOCHS} loss={round_(total_loss)}, loss_layer={round_(total_loss_layer)}, loss_img={round_(total_loss_img)}, loss_cyc={round_(total_loss_cyc)}, epoch_duration={round(epoch_time)} secs')\n",
    "    \n",
    "    wandb.log({\n",
    "        'loss':total_loss,\n",
    "        'loss_layer':total_loss_layer,\n",
    "        'loss_img':total_loss_img,\n",
    "        'loss_cyc':total_loss_cyc,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    \n",
    "    if lr_sched is not None and optimizer.param_groups[0]['lr'] > config_dict['min_lr']:\n",
    "        lr_sched.step()\n",
    "    \n",
    "    if total_loss < prev_best_loss:\n",
    "        prev_best_loss = total_loss\n",
    "        state = {\n",
    "            'model_state': inv_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_sched': lr_sched.state_dict() if lr_sched else None,\n",
    "            'epoch': epoch,\n",
    "            'loss': prev_best_loss,\n",
    "            'prev_best_loss': prev_best_loss\n",
    "        }\n",
    "        torch.save(state, run_path/'state_3layer_inv_nn.pt')\n",
    "        print(f'new best loss={total_loss}')\n",
    "\n",
    "        \n",
    "state = {\n",
    "    'model_state': inv_model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_sched': lr_sched.state_dict() if lr_sched else None,\n",
    "    'epoch': epoch,\n",
    "    'loss': total_loss,\n",
    "    'prev_best_loss': prev_best_loss\n",
    "    }\n",
    "torch.save(state, state_path)\n",
    "        \n",
    "print(f'{EPOCHS} epochs finished in {total_training_time/60} mins')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'model_state': inv_model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_sched': lr_sched.state_dict() if lr_sched else None,\n",
    "    'epoch': epoch,\n",
    "    'loss': total_loss,\n",
    "    'prev_best_loss': prev_best_loss\n",
    "    }\n",
    "torch.save(state, state_path)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and compare the original vs generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [val.size() for val in model.forward_vals], [val.size() for val in inv_model.forward_vals], len(model.layers), len(inv_model.layers), inv_model.first_conv_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "state = torch.load(run_path/'state_small_inv_nn.pt')\n",
    "inv_model.load_state_dict(state['model_state'])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "total_plts = 10\n",
    "for i in range(total_plts):\n",
    "    _ = np.random.randint(len(test_qmnist))\n",
    "    inp = model(test_qmnist[_][0].unsqueeze(0).to(device))\n",
    "    o = inv_model(inp)\n",
    "    img = o.reshape((28, 28)).cpu().detach().numpy()\n",
    "    ax_org = plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(test_qmnist[_][0].squeeze().numpy(), cmap='gray')\n",
    "    ax_pred = plt.subplot(2, 10, i+1+total_plts) \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    label = torch.argmax(model(o))\n",
    "    ax_pred.set_title(f'pred={label}')\n",
    "    ax_org.set_title(f'tgt={test_qmnist[_][1]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "state = torch.load(run_path/'state_4layer_inv_nn.pt')\n",
    "inv_model.load_state_dict(state['model_state'])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "total_plts = 10\n",
    "for i in range(total_plts):\n",
    "    _ = np.random.randint(len(test_qmnist))\n",
    "    inp = model(train_qmnist[_][0].unsqueeze(0).to(device))\n",
    "    label = torch.argmax(inp)\n",
    "    o = inv_model(inp)\n",
    "    img = o.reshape((28, 28)).cpu().detach().numpy()\n",
    "    ax_org = plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(train_qmnist[_][0].squeeze().numpy(), cmap='gray')\n",
    "    ax_pred = plt.subplot(2, 10, i+1+total_plts)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    ax_pred.set_title(f'pred={label}')\n",
    "    ax_org.set_title(f'tgt={train_qmnist[_][1]}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(run_path/f'generated_{str(datetime.now())}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how identifiable the images generated using reversed model are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tgts = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i, test_sample in enumerate(tqdm(test_qmnist)):\n",
    "        inp = model(test_qmnist[i][0].unsqueeze(0).to(device))\n",
    "        tgt = test_qmnist[i][1]\n",
    "        o = inv_model(inp)  # generate image using inverted model\n",
    "        pred = torch.argmax(torch.softmax(model(o), -1))\n",
    "        tgts.append(tgt)\n",
    "        preds.append(pred.item())\n",
    "\n",
    "print(f'accuracy on generated images = {(np.array(tgts) == np.array(preds)).sum()/len(tgts) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# state = torch.load(run_path/'state_3layer_inv_nn.pt')\n",
    "# inv_model.load_state_dict(state['model_state'])\n",
    "\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# total_plts = 10\n",
    "# for i in range(total_plts):\n",
    "#     _ = np.random.randint(len(train_qmnist))\n",
    "#     inp = model(train_qmnist[_][0].to(device))\n",
    "#     o = inv_model(inp)\n",
    "#     img = o.squeeze().cpu().detach().numpy()\n",
    "#     ax_org = plt.subplot(2, 10, i+1)\n",
    "#     plt.imshow(train_qmnist[_][0].squeeze().numpy(), cmap='gray')\n",
    "#     ax_pred = plt.subplot(2, 10, i+1+total_plts) \n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     label = torch.argmax(model(o))\n",
    "#     ax_pred.set_title(f'pred={label}')\n",
    "#     ax_org.set_title(f'tgt={train_qmnist[_][1]}')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
