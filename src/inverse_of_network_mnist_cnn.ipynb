{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep Neural Networks are Surprisingly Reversible](https://arxiv.org/abs/2107.06304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables for input/output paths based on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_RfbDW775Mwf",
    "outputId": "c97ece70-ebfa-4a2a-df1e-1cf14e4d7f0e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# from pathlib import Path\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# DRIVE_PATH = Path('/content/drive/My Drive/ML_Training_Data/inverse-of-neural-net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3I8oIVW5LPW"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_PATH = Path('../inputs/mnist/cnn')\n",
    "OUTPUT_PATH = Path('../outputs/mnist/cnn')\n",
    "\n",
    "# if training on colab with Drive storage\n",
    "# INPUT_PATH = DRIVE_PATH / 'inputs/mnist/cnn'\n",
    "# OUTPUT_PATH = DRIVE_PATH / 'outputs/mnist/cnn'\n",
    "\n",
    "INPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the \"target\" model which we want to reverse/invert. Each layer used in this model will have a corresponding layer in the inverted model, which should perform the \"reverse\" of what target model layer does. Some layers like MaxPool layers are tricky, need to try those next. For now, I have skipped pooling here.\n",
    "\n",
    "The purpose of the paper was to be able to find a reverse of a target Neural Net so that the inputs on which it was trained on can be recovered (check the paper for motivation behind it), assuming that we don't have access to original dataset.\n",
    "\n",
    "That's why here we will train the model on MNIST dataset, but train the inverted model on QMNIST. But in the paper, authors have used a sophisticated way to generated small subset of input images with same statistics are original imputs. But, for this small demo I will use existing MNIST-like datasets already generated by researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4EvVyZd5LPZ",
    "outputId": "88cd7607-9e36-4694-8b88-6ef1df7b6ecf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(999)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, in_channel, out_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 4 layers config\n",
    "        layer1 = self.get_conv_leakyrelu(in_channel, 5, 2, stride=2, slope=0.2)\n",
    "        layer2 = self.get_conv_leakyrelu(5, 10, 2, stride=1, slope=0.2)\n",
    "        layer1_out = layer1(torch.rand((1, 1, 28, 28)))\n",
    "        layer2_out = layer2(layer1_out)\n",
    "        print(f'conv outs={layer1_out.size()}, {layer2_out.size()}')\n",
    "        layer3 = self.get_linear_with_leakyrelu(layer2_out.flatten(1).size(-1), int(layer2_out.flatten(1).size(-1)/2), 0.2)\n",
    "        layer4 = nn.Linear(int(layer2_out.flatten(1).size(-1)/2), out_size)\n",
    "        self.layers = nn.ModuleList([layer1, layer2, layer3, layer4])\n",
    "        \n",
    "        # 3 layers config\n",
    "        # layer1 = self.get_conv_relu(in_channel, 20, 3)\n",
    "        # layer1_out = layer1(torch.rand((1, 1, 28, 28)))\n",
    "        # print(f'conv outs={layer1_out.size()}')\n",
    "        # layer2 = self.get_linear_with_relu(layer1_out.flatten(1).size(-1), int(layer1_out.flatten(1).size(-1)/2))\n",
    "        # layer3 = nn.Linear(int(layer1_out.flatten(1).size(-1)/2), out_size)\n",
    "        # self.layers = nn.ModuleList([layer1, layer2, layer3])\n",
    "\n",
    "        self.forward_vals = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.forward_vals.clear()\n",
    "        for layer in self.layers:\n",
    "            _ = layer[0] if isinstance(layer, nn.Sequential) else layer\n",
    "            if isinstance(_, nn.Linear) and len(x.size()) > 2:\n",
    "                x = x.flatten(1)\n",
    "            x = layer(x)\n",
    "            self.forward_vals.append(x)\n",
    "        return x\n",
    "    \n",
    "    def get_conv_relu_maxpool(self, in_channel, num_filters, kernel_size, pool_size):\n",
    "        return nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size),\n",
    "                             nn.MaxPool2d(pool_size),\n",
    "                             nn.ReLU())\n",
    "        \n",
    "    def get_conv_relu(self, in_channel, num_filters, kernel_size):\n",
    "        return nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size),\n",
    "                             nn.ReLU())\n",
    "        \n",
    "    def get_conv_leakyrelu(self, in_channel, num_filters, kernel_size, stride=1, slope=0.2):\n",
    "        return nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size, stride=stride),\n",
    "                             nn.LeakyReLU(slope))\n",
    "\n",
    "    def get_linear_with_relu(self, inp, out):\n",
    "        return nn.Sequential(nn.Linear(inp, out), nn.ReLU())\n",
    "    \n",
    "    def get_linear_with_leakyrelu(self, inp, out, neg_slope):\n",
    "        return nn.Sequential(nn.Linear(inp, out), nn.LeakyReLU(neg_slope))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "train_kwargs = {'batch_size': batch_size,\n",
    "               'shuffle': True}\n",
    "test_kwargs = {'batch_size': batch_size,\n",
    "              'shuffle': True}\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "dataset1 = datasets.MNIST(INPUT_PATH/'mnist', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST(INPUT_PATH/'mnist', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net(28, 1, 10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "model, model(torch.rand(2, 1, 28, 28).to(device)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "dBoJi7PG5LPc",
    "outputId": "a92b2e43-8a12-41b9-c81a-e756c118a365",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seIGN3p75LPc"
   },
   "source": [
    "### Target Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4wf5nX85LPd"
   },
   "source": [
    "If you have not already trained a model, uncomment the next cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20EWoLxY5LPd",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# def train(model, device, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         # print(data.size(), data.view(data.size(0), -1).size())\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = F.cross_entropy(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "# def test():\n",
    "#     with torch.no_grad():\n",
    "#         n_correct = 0\n",
    "#         n_samples = 0\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             # max returns (value ,index)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             n_samples += labels.size(0)\n",
    "#             n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     acc = 100.0 * n_correct / n_samples\n",
    "#     print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "#     return acc\n",
    "\n",
    "\n",
    "# best_acc = -1\n",
    "# best_model = None\n",
    "# for epoch in range(1, 50):\n",
    "#     train(model, device, train_loader, optimizer, epoch)\n",
    "#     acc = test()\n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         best_model = copy.deepcopy(model)\n",
    "#         print(f'new best acc={best_acc}')\n",
    "#     else:\n",
    "#         print(f'current acc={acc}, prev_best_acc={best_acc}')\n",
    "\n",
    "# state = {\n",
    "#     'model_state': best_model.state_dict(),\n",
    "#     'test_acc': best_acc\n",
    "# }\n",
    "# torch.save(state, OUTPUT_PATH/'state_4layer_noPool_nn.pt')\n",
    "# print('best model saved')\n",
    "# # 98.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFT2Um835LPe"
   },
   "source": [
    "### Reverse Model Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the saved best target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtqY_OgN5LPf",
    "outputId": "8d9cee92-aaa4-48cc-bea5-b88f018f5a19"
   },
   "outputs": [],
   "source": [
    "state = torch.load(OUTPUT_PATH/'state_4layer_noPool_nn.pt')\n",
    "state.keys(), state['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hMAxRrW5LPf",
    "outputId": "5db7b28c-bc11-414b-fad5-db0395a7eecf"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(state['model_state'])\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "                                 \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations which are used in the current experiment/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI44icM15LPf"
   },
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'epochs': 600,\n",
    "    'alpha': 1,\n",
    "    'lr': 1e-3,\n",
    "    'lr_sched': 'exp',\n",
    "    'gamma': 0.99,\n",
    "    'loss': 'abs',\n",
    "    'min_lr': 1e-5,\n",
    "    'max_lr': 1e-4,\n",
    "    'leaky_relu': 0.2,\n",
    "    'actv': 'leaky_relu',\n",
    "    'use_bn': False,\n",
    "    'batch_size': 128,\n",
    "    'init_method': 'lecun_normal',\n",
    "    'optim': 'adam',\n",
    "    'momentum': 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a different dataset, you may want to normalize it with appropriate stats. For MNIST I have used constant figure from internet directly (which is close to mean and std already). In the code, the usage of get_stats() is commented out to show how it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddEOwquY5LPg"
   },
   "outputs": [],
   "source": [
    "def get_stats():\n",
    "    train_qmnist = datasets.QMNIST(INPUT_PATH/'qmnist', train = True, download= True,\n",
    "                             transform=transforms.ToTensor())\n",
    "    dataloader = torch.utils.data.DataLoader(train_qmnist, batch_size=128)\n",
    "    nimages = 0\n",
    "    mean = 0.0\n",
    "    var = 0.0\n",
    "    for i_batch, batch_target in enumerate(dataloader):\n",
    "        batch = batch_target[0]\n",
    "        # Rearrange batch to be the shape of [B, C, W * H]\n",
    "        batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "        # Update total number of images\n",
    "        nimages += batch.size(0)\n",
    "        # Compute mean and std here\n",
    "        mean += batch.mean(2).sum(0) \n",
    "        var += batch.var(2).sum(0)\n",
    "\n",
    "    mean /= nimages\n",
    "    var /= nimages\n",
    "    std = torch.sqrt(var)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUZ2frQV5LPg",
    "outputId": "c8541866-57c0-49f3-b18a-3d0618749d38"
   },
   "outputs": [],
   "source": [
    "model(torch.rand((1, 1, 28, 28)).to(device))\n",
    "[print(val.size()) for val in model.forward_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBfvsxbk5LPg",
    "outputId": "a31c6338-b9dc-4438-d115-a9febdd46ef9"
   },
   "outputs": [],
   "source": [
    "from torch.nn.init import xavier_normal_, xavier_uniform_\n",
    "from torch.nn.init import zeros_, uniform_, _calculate_correct_fan, normal_\n",
    "from torch.nn.init import kaiming_uniform_, kaiming_normal_\n",
    "\n",
    "class InvNet(nn.Module):\n",
    "    def __init__(self, out_dim, out_channel, tgt_model):\n",
    "        super(InvNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i, tgt_layer in enumerate(tgt_model.layers[::-1], 1):\n",
    "            if isinstance(tgt_layer, nn.Sequential):\n",
    "                tgt_layer = tgt_layer[0]\n",
    "            \n",
    "            if isinstance(tgt_layer, nn.Linear):\n",
    "                if len(tgt_model.forward_vals[-i-1].size()) > 2:\n",
    "                    out_size = tgt_model.forward_vals[-i-1].flatten(1).size(-1)\n",
    "                    self.first_conv_dim = tgt_model.forward_vals[-i-1].size()\n",
    "                else:\n",
    "                    out_size = tgt_model.forward_vals[-i-1].size(-1)\n",
    "                \n",
    "                # use this var to only use activations based on some conditions, here always True\n",
    "                use_actv = True\n",
    "                layer = self.get_linear_with_relu(tgt_model.forward_vals[-i].size(-1), out_size, config_dict['use_bn'], use_actv)\n",
    "            elif isinstance(tgt_layer, nn.Conv2d):\n",
    "                if i < len(tgt_model.layers):\n",
    "                    out_channel = tgt_model.forward_vals[-i-1].size(1)\n",
    "                    layer = self.get_conv_relu(tgt_model.forward_vals[-i].size(1), out_channel, 2, 1, 1, 1, \n",
    "                                               True if len(config_dict['actv'])>0 else False,\n",
    "                                              config_dict['use_bn'])\n",
    "                else:\n",
    "                    # last layer\n",
    "                    out_channel = 1\n",
    "                    layer = self.get_upsample_and_conv(tgt_model.forward_vals[-i].size(1), out_channel, 2, 1, 0, 1, False, False)\n",
    "                \n",
    "            layers.append(layer)\n",
    "\n",
    "        self.conv_type = type(layers[-2][0])\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.forward_vals = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.forward_vals.clear()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer[0], self.conv_type) and len(x.size()) <= 2:\n",
    "                x = x.view(x.size(0), self.first_conv_dim[1], self.first_conv_dim[2], self.first_conv_dim[3])  # out dims of conv2d of target model\n",
    "            x = layer(x)\n",
    "            self.forward_vals.append(x)\n",
    "        return x\n",
    "    \n",
    "    def get_conv_relu(self, in_channel, num_filters, kernel_size, stride, padding, dilation, use_actv=True, use_bn=False):\n",
    "        sequential = nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size, \n",
    "                                                    stride=stride, padding=padding, dilation=dilation,\n",
    "                                                    bias=True))\n",
    "        \n",
    "        if use_bn:\n",
    "            sequential.add_module('bn', nn.BatchNorm2d(num_filters))\n",
    "        \n",
    "        if use_actv:\n",
    "            sequential.add_module('actv', self.get_activation(config_dict['actv']))\n",
    "            \n",
    "        return sequential\n",
    "\n",
    "    def get_upsample_and_conv(self, in_channel, num_filters, kernel_size, stride, padding, dilation, use_actv=True, use_bn=False):\n",
    "        sequential = nn.Sequential(nn.Upsample(size=29, mode='bilinear'),\n",
    "            nn.Conv2d(in_channel, num_filters, kernel_size,\n",
    "                                                    stride=stride, padding=padding, dilation=dilation,\n",
    "                                                    bias=True)\n",
    "            )\n",
    "        \n",
    "        if use_bn:\n",
    "            sequential.add_module('bn', nn.BatchNorm2d(num_filters))\n",
    "        \n",
    "        if use_actv:\n",
    "            sequential.add_module('actv', self.get_activation(config_dict['actv']))\n",
    "            \n",
    "        return sequential\n",
    "\n",
    "    def get_conv_and_upsample(self, in_channel, num_filters, kernel_size, stride, padding, dilation, use_actv=True, use_bn=False):\n",
    "        sequential = nn.Sequential(nn.Conv2d(in_channel, num_filters, kernel_size,\n",
    "                                            stride=stride, padding=padding, dilation=dilation,\n",
    "                                            bias=True),\n",
    "                                   nn.Upsample(size=28, mode='bilinear')\n",
    "                                   )\n",
    "        \n",
    "        if use_bn:\n",
    "            sequential.add_module('bn', nn.BatchNorm2d(num_filters))\n",
    "        \n",
    "        if use_actv:\n",
    "            sequential.add_module('actv', self.get_activation(config_dict['actv']))\n",
    "            \n",
    "        return sequential\n",
    "\n",
    "    def get_activation(self, actv):\n",
    "        if actv == 'leaky_relu':\n",
    "            return nn.LeakyReLU(config_dict['leaky_relu'])\n",
    "        elif actv == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif actv == 'selu':\n",
    "            return nn.SELU()\n",
    "        elif actv == 'tanh':\n",
    "            return nn.Tanh()\n",
    "    \n",
    "    def get_linear_with_relu(self, inp, out, use_bn, use_actv=True):\n",
    "        sequential =  nn.Sequential(nn.Linear(inp, out))\n",
    "        if use_bn:\n",
    "            sequential.add_module('lin_bn', nn.BatchNorm1d(out))\n",
    "        if use_actv:\n",
    "            sequential.add_module('lin_actv', self.get_activation(config_dict['actv']))\n",
    "        return sequential\n",
    "\n",
    "\n",
    "def lecun_normal_(weight, mode):\n",
    "    fan = _calculate_correct_fan(weight, mode)\n",
    "    limit = torch.sqrt(3/torch.tensor(fan))\n",
    "    normal_(weight, -limit, limit)\n",
    "\n",
    "\n",
    "def init_weights(layer):\n",
    "    def init_(data):\n",
    "        if config_dict['init_method'] == 'he_normal':\n",
    "            kaiming_normal_(data, a=config_dict['leaky_relu'], nonlinearity='selu')\n",
    "        elif config_dict['init_method'] == 'xavier_normal':\n",
    "            xavier_normal_(data)\n",
    "        elif config_dict['init_method'] == 'lecun_normal':\n",
    "            lecun_normal_(data, 'fan_in') \n",
    "\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        init_(layer.weight)\n",
    "        # zeros_(layer.bias)\n",
    "    elif isinstance(layer, nn.Conv2d):\n",
    "        init_(layer.weight)\n",
    "        # zeros_(layer.bias)\n",
    "\n",
    "\n",
    "inv_model = InvNet(28, 1, model)\n",
    "inv_model.apply(init_weights)\n",
    "inv_model.to(device)\n",
    "# mean, std = get_stats()\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)) # or use mean, std here\n",
    "        ])\n",
    "\n",
    "batch_size = config_dict['batch_size']\n",
    "test_kwargs = {'batch_size': batch_size,\n",
    "              'shuffle': True}\n",
    "train_qmnist = datasets.QMNIST(INPUT_PATH/'qmnist', train = True, download= True,\n",
    "                             transform=transform) \n",
    "test_qmnist = datasets.QMNIST(INPUT_PATH/'qmnist', train = False, download= True,\n",
    "                             transform=transform)\n",
    "\n",
    "print(f'size of inv data={train_qmnist}')\n",
    "inv_data_loader = torch.utils.data.DataLoader(train_qmnist, **test_kwargs)\n",
    "\n",
    "if config_dict['loss'] == 'mse':\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif config_dict['loss'] == 'huber':\n",
    "    loss_fn = nn.HuberLoss()\n",
    "else:\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "if config_dict['optim'] == 'adam':\n",
    "    optimizer = optim.Adam(inv_model.parameters(), lr=config_dict['lr'])\n",
    "elif config_dict['optim'] == 'sgd':\n",
    "    optimizer = optim.SGD(inv_model.parameters(), lr=config_dict['lr'],\n",
    "                          momentum=config_dict['momentum'], nesterov=True)\n",
    "elif config_dict['optim'] == 'nadam':\n",
    "    optimizer = optim.NAdam(inv_model.parameters(), lr=config_dict['lr'])\n",
    "\n",
    "lr_sched = None\n",
    "if config_dict['lr_sched'] == 'exp':\n",
    "    lr_sched = optim.lr_scheduler.ExponentialLR(optimizer, config_dict['gamma'], verbose=False)\n",
    "\n",
    "print(inv_model,'\\n', loss_fn, lr_sched)\n",
    "o = inv_model(torch.rand((2, 10)).to(device))\n",
    "print(f'out_size={o.size()}')\n",
    "[val.size() for val in inv_model.forward_vals], inv_model.first_conv_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "MpNjyLri5LPi",
    "outputId": "8ebc1cb0-860f-4469-fc33-51177bee8086",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "for i in range(6):\n",
    "    example_data, example_target = test_qmnist[random.randint(0, len(test_qmnist))]\n",
    "    ax = plt.subplot(2,3,i+1)\n",
    "    ax.set_title(f'label={example_target}')\n",
    "    plt.imshow(example_data.squeeze(), cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fsF3hMv5LPi"
   },
   "outputs": [],
   "source": [
    "def get_layer_map(model):\n",
    "    layer_map = {}\n",
    "    for i in range(len(model.layers)):\n",
    "        layer_map[i] = len(model.layers)-(i+1)\n",
    "    print(f'layer_map={layer_map}')\n",
    "    return layer_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-1qFJLf5LPi"
   },
   "outputs": [],
   "source": [
    "def layer_wise_loss(model, inv_model, inp, k=-1):\n",
    "    loss = 0\n",
    "    for layer_num, layer in enumerate(model.layers[:k+1] if k > -1 else model.layers):\n",
    "        tgt_layer_in = inp if layer_num == 0 else model.forward_vals[layer_num-1]  # get the input tensor for this layer in tgt model\n",
    "        tgt_layer_out = model.forward_vals[layer_num]  # get the output of i-th target model layer \n",
    "        inv_layer = inv_model.layers[tgt_to_inv_layer_map[layer_num]]  # get the corresponding layer in reverse model\n",
    "        inv_layer_out = inv_layer(tgt_layer_out)  # feed the target model layer output to reverse model layer\n",
    "        if inv_layer_out.size(-1) != tgt_layer_in.size(-1):\n",
    "            tgt_layer_in = tgt_layer_in.flatten(1)\n",
    "        loss += loss_fn(inv_layer_out, tgt_layer_in)  # outputs of both layer should be similar\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def layer_wise_loss_progressive(model, inv_model, inp, layer_num):\n",
    "    tgt_layer_in = inp if layer_num == 0 else model.forward_vals[layer_num-1]  # get the input tensor for this layer in tgt model\n",
    "    tgt_layer_out = model.forward_vals[layer_num]  # get the output of i-th target model layer \n",
    "    inv_layer = inv_model.layers[tgt_to_inv_layer_map[layer_num]]  # get the corresponding layer in reverse model\n",
    "    inv_layer_out = inv_layer(tgt_layer_out)  # feed the target model layer output to reverse model layer\n",
    "    if inv_layer_out.size(-1) != tgt_layer_in.size(-1):\n",
    "        tgt_layer_in = tgt_layer_in.flatten(1)\n",
    "    return loss_fn(inv_layer_out, tgt_layer_in)  # outputs of both layer should be similar\n",
    "\n",
    "\n",
    "def k_layer_loss(model, inv_model, inp, k=-1):\n",
    "    loss = 0\n",
    "    for layer_num, layer_ in enumerate(model.layers[:k+1] if k > -1 else model.layers):\n",
    "        tgt_layer_out = model.forward_vals[layer_num]\n",
    "        x = tgt_layer_out\n",
    "        for layer in inv_model.layers[-(1+layer_num):]:\n",
    "            if isinstance(layer[0], inv_model.conv_type) and len(x.size()) < 4:\n",
    "                x = x.view(x.size(0), inv_model.first_conv_dim[1], inv_model.first_conv_dim[2], inv_model.first_conv_dim[3])\n",
    "            x = layer(x)\n",
    "        loss += loss_fn(x, inp)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def k_layer_loss_progressive(model, inv_model, inp, layer_num):\n",
    "    tgt_layer_out = model.forward_vals[layer_num]\n",
    "    x = tgt_layer_out\n",
    "    for layer in inv_model.layers[-(1+layer_num):]:\n",
    "        if isinstance(layer[0], inv_model.conv_type) and len(x.size()) < 4:\n",
    "            x = x.view(x.size(0), inv_model.first_conv_dim[1], inv_model.first_conv_dim[2], inv_model.first_conv_dim[3])\n",
    "        x = layer(x)\n",
    "    return loss_fn(x, inp)\n",
    "\n",
    "\n",
    "def cycle_consistency_inversion_loss(model, inp, inv_out, k=-1):\n",
    "    org_activations = copy.deepcopy(model.forward_vals[:k+1] if k > -1 else model.forward_vals)\n",
    "    out_for_inv_input = model(inv_out)  # pass the input image generated by inverted network\n",
    "    inv_activations = model.forward_vals[:k+1] if k > -1 else model.forward_vals\n",
    "    loss = 0\n",
    "    for org_actv, actv in zip(org_activations, inv_activations):\n",
    "        loss += loss_fn(actv, org_actv)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgsJU62T-8ms"
   },
   "source": [
    "### all loss together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCrx37R1-6jg",
    "outputId": "6f16aa02-a206-4e46-a7d6-9a8665ff63f1"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "tgt_to_inv_layer_map = get_layer_map(model)\n",
    "\n",
    "def train_inv(model, inv_model, device, data_loader, optimizer, epoch, alpha, layer_wise_only=False, layer_num=-1):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_loss_layer = 0\n",
    "    total_loss_img = 0\n",
    "    total_loss_cyc = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        out = model(data)  \n",
    "        inv_out = inv_model(out)\n",
    "        \n",
    "        # step 1- layer wise load\n",
    "        loss_layer = layer_wise_loss(model, inv_model, data)\n",
    "        \n",
    "        if not layer_wise_only:\n",
    "            # step 2- upto layer-k loss\n",
    "            loss_img = k_layer_loss(model, inv_model, data)\n",
    "\n",
    "            # step 3- full network output loss\n",
    "            loss_cyc = cycle_consistency_inversion_loss(model, data, inv_out)\n",
    "        else:\n",
    "            loss_img = torch.tensor(0)\n",
    "            loss_cyc = torch.tensor(0)\n",
    "        \n",
    "        loss = loss_layer + loss_img + (alpha*loss_cyc)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_loss_layer += loss_layer.item()\n",
    "        total_loss_img += loss_img.item()\n",
    "        total_loss_cyc += loss_cyc.item()\n",
    "        \n",
    "    return total_loss / (batch_idx+1), total_loss_layer / (batch_idx+1), \\\n",
    "            total_loss_img / (batch_idx+1), total_loss_cyc / (batch_idx+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpS9awQRfpW9"
   },
   "source": [
    "### Progressive optimization of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZqN3U2TfucF",
    "outputId": "aa192142-1fdc-49aa-dfbf-ffc0e1c92a92"
   },
   "outputs": [],
   "source": [
    "tgt_to_inv_layer_map = get_layer_map(model)\n",
    "\n",
    "def train_inv_progressive(model, inv_model, device, data_loader, optimizer, epoch, alpha, layer_wise_only=False):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_loss_layer = 0\n",
    "    total_loss_img = 0\n",
    "    total_loss_cyc = 0\n",
    "    \n",
    "    for layer_num in range(len(model.layers)):\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(data)\n",
    "            org_activations = copy.deepcopy(model.forward_vals) \n",
    "            inv_out = inv_model(out)\n",
    "            out_for_inv = model(inv_out)\n",
    "\n",
    "            loss = layer_wise_loss_progressive(model, inv_model, data, layer_num) + \\\n",
    "                    k_layer_loss_progressive(model, inv_model, data, layer_num) + \\\n",
    "                    (alpha * loss_fn(model.forward_vals[layer_num], org_activations[layer_num]))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # this means I am not training inv_model conv2d with these losses!! is it an issue?\n",
    "        # also, does \"fine-tune\" mean, after training a inv_layer k, all layers 1..k are taken as single module and optimized again using L_k?s\n",
    "        if layer_num > 0:\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = model(data)  \n",
    "                inv_out = inv_model(out)\n",
    "\n",
    "                loss_layer = layer_wise_loss(model, inv_model, data, layer_num)\n",
    "                loss_img = k_layer_loss(model, inv_model, data, layer_num)\n",
    "                loss_cyc = cycle_consistency_inversion_loss(model, data, inv_out, layer_num)\n",
    "\n",
    "                loss = loss_layer + loss_img + (alpha*loss_cyc)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_loss_layer += loss_layer.item()\n",
    "                total_loss_img += loss_img.item()\n",
    "                total_loss_cyc += loss_cyc.item()\n",
    "    \n",
    "    div = (batch_idx+1)*(len(inv_model.layers)-1)\n",
    "    return total_loss / div, total_loss_layer / div, \\\n",
    "            total_loss_img / div, total_loss_cyc / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXe_E5U_5LPj",
    "outputId": "1e3743a8-8365-45da-cada-21ca6666b799"
   },
   "outputs": [],
   "source": [
    "run_id = 'temp'# 'newProgressive_noPoolInTgt_4layer_nonProgsv'\n",
    "change_desc = 'non progressive training, tgt_model 4 layered lrelu without maxPool, inv_model with lrelu & upsample+conv'\n",
    "is_resume = True\n",
    "run_path = OUTPUT_PATH/'runs'/run_id\n",
    "state_path = run_path/'final_state_4layer_noTgtPool_inv_nn.pt'\n",
    "best_model_path = run_path/'state_4layer_noTgtPool_inv_nn.pt'\n",
    "\n",
    "# used to log progress. not mandatory, just need to comment out \"wandb\" if not needed\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9HSRg-UQRvW"
   },
   "source": [
    "### Reverse Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TJcplO3Y5LPj",
    "outputId": "7592da68-fcc9-4864-c363-c06b5a8f8b8f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def round_(n, d=5):\n",
    "    return np.round(n, d)\n",
    "\n",
    "\n",
    "run_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EPOCHS = config_dict['epochs']\n",
    "ALPHA = config_dict['alpha']\n",
    "start_epoch = 0\n",
    "prev_best_loss = 999999\n",
    "total_training_time = 0\n",
    "\n",
    "if is_resume and state_path.exists():\n",
    "    state = torch.load(state_path)\n",
    "    inv_model.load_state_dict(state['model_state'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    if lr_sched:\n",
    "        lr_sched.load_state_dict(state['lr_sched'])\n",
    "    start_epoch = state['epoch']+1\n",
    "    prev_best_loss = state['prev_best_loss']\n",
    "\n",
    "    # uncomment, if you want to override the saved previous lr\n",
    "    # for g in optimizer.param_groups:\n",
    "    #     g['lr'] = config_dict['lr']\n",
    "\n",
    "    print(f'resuming training for {run_id} from epoch {start_epoch}, last loss {state[\"loss\"]} prev_best_loss {prev_best_loss}')\n",
    "    \n",
    "\n",
    "wandb.init(project='nn-are-reversible',\n",
    "           entity='nayash', save_code=True, id=run_id,\n",
    "           name=run_id, notes=change_desc,\n",
    "           dir=run_path,\n",
    "           resume='allow',\n",
    "           config=config_dict)\n",
    "wandb.watch(inv_model)\n",
    "wandb.log({'model_arch': str(inv_model)})\n",
    "\n",
    "if run_id in ['test', 'temp']:\n",
    "    os.environ['WANDB_MODE'] = 'offline'\n",
    "    print(f'run_id={run_id}, so wandb is disabled!')\n",
    "          \n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    stime = time.time()\n",
    "    total_loss, total_loss_layer, total_loss_img, total_loss_cyc = \\\n",
    "        train_inv(model, inv_model, device, inv_data_loader, optimizer, epoch, ALPHA)\n",
    "\n",
    "    epoch_time = time.time() - stime\n",
    "    total_training_time += epoch_time\n",
    "    \n",
    "    print(f'epoch={epoch}/{EPOCHS} loss={round_(total_loss)}, loss_layer={round_(total_loss_layer)}, loss_img={round_(total_loss_img)}, loss_cyc={round_(total_loss_cyc)}, epoch_duration={round(epoch_time)} secs')\n",
    "    \n",
    "    wandb.log({\n",
    "        'loss':total_loss,\n",
    "        'loss_layer':total_loss_layer,\n",
    "        'loss_img':total_loss_img,\n",
    "        'loss_cyc':total_loss_cyc,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    \n",
    "    if lr_sched is not None and optimizer.param_groups[0]['lr'] > config_dict['min_lr']:\n",
    "        lr_sched.step()\n",
    "    \n",
    "    if total_loss < prev_best_loss:\n",
    "        prev_best_loss = total_loss\n",
    "        state = {\n",
    "            'model_state': inv_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_sched': lr_sched.state_dict() if lr_sched else None,\n",
    "            'epoch': epoch,\n",
    "            'loss': prev_best_loss,\n",
    "            'prev_best_loss': prev_best_loss\n",
    "        }\n",
    "        torch.save(state, best_model_path)\n",
    "        print(f'new best loss={total_loss}')\n",
    "\n",
    "        \n",
    "state = {\n",
    "    'model_state': inv_model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_sched': lr_sched.state_dict() if lr_sched else None,\n",
    "    'epoch': epoch,\n",
    "    'loss': total_loss,\n",
    "    'prev_best_loss': prev_best_loss\n",
    "    }\n",
    "torch.save(state, state_path)\n",
    "        \n",
    "print(f'{EPOCHS} epochs finished in {total_training_time/60} mins')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell, if the above \"training\" cell was stopped half way to save the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328,
     "referenced_widgets": [
      "de848e74e7a84e1f8114b02d05448627",
      "9f1e65a0647c493d83a3f97d4afb055d",
      "23cef86aa5c94eccbd4990d312de2f2b",
      "e84b0f9f707b438c87e26134a4556459",
      "6369f25df8b443c082062838b7ae2268",
      "3d4aca8cf3794d428a636a82dc55c963",
      "c500fc1eb1ef46aea382c79fe6f2ebce",
      "3b390e7ce2f1416a817a3ae9cfe1915c"
     ]
    },
    "id": "AvoNR-Q-5LPj",
    "outputId": "d0869a07-11c5-4b2f-9d6a-1970770e6c57"
   },
   "outputs": [],
   "source": [
    "state = {\n",
    "    'model_state': inv_model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_sched': lr_sched.state_dict() if lr_sched else None,\n",
    "    'epoch': epoch,\n",
    "    'loss': total_loss,\n",
    "    'prev_best_loss': prev_best_loss\n",
    "    }\n",
    "torch.save(state, state_path)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyJa76Cw5LPj"
   },
   "source": [
    "Plot and compare the original vs generated images (1st and 2nd row). 3rd and 4th row plot the outputs for any target intermediate layer and the input for the corresponding inverted layer, to validate that the activations are acutally similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "CAqcgbei5LPk",
    "outputId": "0cd25c53-ff53-4c2b-8d92-9916818222f8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "state = torch.load(best_model_path)\n",
    "inv_model.load_state_dict(state['model_state'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "total_plts = 10\n",
    "\n",
    "for i in range(total_plts):\n",
    "    _ = np.random.randint(len(test_qmnist))\n",
    "    inp = model(test_qmnist[_][0].unsqueeze(0).to(device))\n",
    "    o = inv_model(inp)\n",
    "    img = o.reshape((28, 28)).cpu().detach().numpy()\n",
    "    ax_org = plt.subplot(4, 10, i+1)\n",
    "    plt.imshow(test_qmnist[_][0].squeeze().numpy(), cmap='gray')\n",
    "    ax_pred = plt.subplot(4, 10, i+1+total_plts)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    label = torch.argmax(model(o))\n",
    "    ax_pred.set_title(f'pred={label}')\n",
    "    ax_org.set_title(f'tgt={test_qmnist[_][1]}')\n",
    "    \n",
    "    ax_tgt_fmap = plt.subplot(4, 10, i+1+2*total_plts)\n",
    "    # view the outputs of first conv layer\n",
    "    plt.imshow(model.forward_vals[1].squeeze().mean(0).cpu().detach().numpy(), cmap='gray')\n",
    "    ax_inv_fmap = plt.subplot(4, 10, i+1+3*total_plts)\n",
    "    # view the inputs to last conv layer of inverted model. It should be close to first tgt conv layers output\n",
    "    plt.imshow(inv_model.forward_vals[1].view(tuple(inv_model.first_conv_dim)).squeeze().mean(0).cpu().detach().numpy(), cmap='gray')\n",
    "    ax_tgt_fmap.set_title('tgt_fmap')\n",
    "    ax_inv_fmap.set_title('inv_fmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "sample_save_path = run_path/f'{run_id}_{time.time()}.png'\n",
    "plt.savefig(sample_save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1ksgVCW5LPk"
   },
   "source": [
    "Let's see if target model is able to classify properly the generated images too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-O7-jPz5LPk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tgts = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i, test_sample in enumerate(tqdm(test_qmnist)):\n",
    "        inp = model(test_qmnist[i][0].unsqueeze(0).to(device))\n",
    "        tgt = test_qmnist[i][1]\n",
    "        o = inv_model(inp)  # generate image using inverted model\n",
    "        pred = torch.argmax(torch.softmax(model(o), -1))\n",
    "        tgts.append(tgt)\n",
    "        preds.append(pred.item())\n",
    "\n",
    "print(f'accuracy on generated images = {(np.array(tgts) == np.array(preds)).sum()/len(tgts) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's see how does the images generated on the training set look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trvFrvze5LPk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "state = torch.load(best_model_path)\n",
    "inv_model.load_state_dict(state['model_state'])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "total_plts = 10\n",
    "for i in range(total_plts):\n",
    "    _ = np.random.randint(len(train_qmnist))\n",
    "    inp = model(train_qmnist[_][0].unsqueeze(0).to(device))\n",
    "    o = inv_model(inp)\n",
    "    img = o.reshape((28, 28)).cpu().detach().numpy()\n",
    "    ax_org = plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(train_qmnist[_][0].squeeze().numpy(), cmap='gray')\n",
    "    ax_pred = plt.subplot(2, 10, i+1+total_plts) \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    label = torch.argmax(model(o))\n",
    "    ax_pred.set_title(f'pred={label}')\n",
    "    ax_org.set_title(f'tgt={train_qmnist[_][1]}')\n",
    "plt.tight_layout()\n",
    "sample_save_path = run_path/f'{run_id}_trainSetSamples_{time.time()}.png'\n",
    "plt.savefig(sample_save_path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "seIGN3p75LPc",
    "fgsJU62T-8ms",
    "yuFHx2lR_DWc",
    "kZaLlyybOGZf"
   ],
   "name": "inverse-of-network-mnist-cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "23cef86aa5c94eccbd4990d312de2f2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d4aca8cf3794d428a636a82dc55c963",
      "placeholder": "​",
      "style": "IPY_MODEL_6369f25df8b443c082062838b7ae2268",
      "value": " 0.44MB of 0.44MB uploaded (0.00MB deduped)\r"
     }
    },
    "3b390e7ce2f1416a817a3ae9cfe1915c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d4aca8cf3794d428a636a82dc55c963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6369f25df8b443c082062838b7ae2268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f1e65a0647c493d83a3f97d4afb055d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c500fc1eb1ef46aea382c79fe6f2ebce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de848e74e7a84e1f8114b02d05448627": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23cef86aa5c94eccbd4990d312de2f2b",
       "IPY_MODEL_e84b0f9f707b438c87e26134a4556459"
      ],
      "layout": "IPY_MODEL_9f1e65a0647c493d83a3f97d4afb055d"
     }
    },
    "e84b0f9f707b438c87e26134a4556459": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b390e7ce2f1416a817a3ae9cfe1915c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c500fc1eb1ef46aea382c79fe6f2ebce",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
